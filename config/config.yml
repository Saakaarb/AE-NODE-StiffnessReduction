data_processing:

    saving_loading:
        load_data: False # True/False
        save_data: True # True/False
        raw_data_path: "raw_data/split_k_4_mild_comb_soln"
        processed_data_path: "processed_data"

    # data processing information
    train_split_ratio: 0.95
    check_raw_data_shape: True
    total_available_features: 5 # the total number of features in the data. This does NOT include time,
                                # assumed to be the first row/column (at index 0)
    feature_train_index: [1,2,3,4,5] #the row/column indices (0 indexed) from the dataset files of which species should be in the training set
                                        # DO NOT include 0 (assumed to be time); this will raise an error
    data_arrange_mode: 'row_major' # row_major: each datapoint is a row in the data files, column_major: each datapoint is a column in the data files
    latent_space_dim: 2
    num_samples_per_batch: 1024

encoder_decoder:
    loading:
        save_model: True # True/False
        load_model: False # True/False
        model_output_dir: 'encoder_decoder_weights'
        load_path_encoder: 'encoder_model.eqx' 
        load_path_decoder: 'decoder_model.eqx' 

    architecture:
        network_type: 'mlp'
        activation_function: 'tanh' # 'relu' or 'gelu' or 'tanh'
        network_width: 128
        num_layers: 3

    training:

        # Optimizer and learning rate
        optimizer: 'adam' # 'adam' or 'l-bfgs'
        start_learning_rate: 0.001 
        end_learning_rate: 0.00001
        learning_rate_decay: 0.9
        learning_rate_decay_steps: 2000

        # batch processing
        num_training_iters: 6000 # for l-bfgs use max 200 steps, for adam use 30000
        
        print_freq: 100 # for l-bfgs use 1 steps, for adam use 100

        # experimental settings
        stiffness_reduction: True # yes/no
        simultaneous_training: False # True/False

        # loss function
        stiffness_reduction_weight: 1E-6 # weight for stiffness reduction loss
    
    testing:
        test_model: True # True/False
        save_dir: "enc_dec_results"
        visualization:
            plot_results: True # True/False
            plot_latent_space: True # True/False
            save_dir: "visualization"
            settings:
                xscale: 'log'

neural_ode:
    saving:
        save_model: True # True/False
        load_model: False # True/False
        model_output_dir: 'NODE_weights'
        load_path: 'NODE_model.eqx' 

    architecture:
        network_type: 'mlp'
        activation_function: 'tanh' # 'relu' or 'gelu' or 'tanh'
        network_width: 128
        num_layers: 3

    training:
       
        # Optimizer and learning rate
        optimizer: 'adam' # 'adam' or 'l-bfgs'
        start_learning_rate: 0.001
        end_learning_rate: 0.0001
        learning_rate_decay: 0.7
        learning_rate_decay_steps: 1000

        precision: 'float32' # 'float32' or 'float64'
                             # if the time scale of the system is very small (< 1E-7), use float64
                             # else diffrax will perform poorly

        # batch processing
        num_training_iters: 1000 # for l-bfgs use max 200 steps, for adam use 10000
        time_scale: 5E-3 
        print_freq: 100

        simultaneous_training: False # True/False

    ode_solver:
        # currently deprecated
        #---------------------------------
        pcoeff: 0.3 # predictor coefficient
        icoeff: 0.4 # corrector coefficient
        rtol: 1e-1 # relative tolerance
        atol: 1e-1 # absolute tolerance
        dtmin: None
        #---------------------------------
        init_dt: 5E-5 # explicit euler integration timestep; make sure this is small enough to capture
                        # the smallest timescale in the system

    testing:
        test_model: True # True/False
        save_dir: "NODE_results"
        visualization:
            plot_results: True # True/False
            save_dir: "visualization"
            settings:
                xscale: 'log'
